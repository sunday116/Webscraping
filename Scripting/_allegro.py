# -*- coding: utf-8 -*-
"""allegro.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uoohOH1oAmDPZs59gK9exLvORqcQe-uQ

### 1.0 Import Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""### 2.0 Helper functions
 
"""

import pandas as pd


def import_data(path):
  try:
    df = pd.read_csv(path, dtype=str)
  except FileNotFoundError as error:
    from google.colab import drive
    drive.mount('/content/gdrive')

    df = pd.read_csv(path, dtype=str)

  print('data imported')

  return df

def export_data(df, path):
  df.to_csv(path)
  print('data exported')

import pandas as pd

path = '/content/drive/Shareddrives/Supplier Intelligence: Tools and Innovations/Compliance Grabber/Manufacturers/molex/Molex_Oct11th.csv'
df = import_data(path)

def add_columns(columns):
  import numpy as np
  
  # check if the column already exists
  for column in columns:
    if not column in df.columns:
      df[column] = np.nan

columns = ['Results', 'status', 'PartNumber', 'partName', 'DataSheets', 'CertificateOfCompliance', 'RoHSCompliant', 'RoHSData']
add_columns(columns)

import os


def save_to_file(url, filename, folder_path):
  #Create a folder if it doesn't exist
  if not os.path.exists(folder_path):
    os.makedirs(folder_path)

  #Download the file
  file_data = requests.get(url).content

  #Save the file under the filename if this file does not exist already
  file_location = os.path.join(folder_path, filename)
  if not os.path.exists(file_location):
    with open(file_location, 'wb') as f:
      f.write(file_data)

  return url

"""### 3.0 Run Compliance Grabber"""

import requests
from bs4 import BeautifulSoup
from bs4.element import NavigableString
import re
import json
from tqdm import tqdm
from urllib.parse import quote_plus


def scrap_allegro(partNumber):
    try:
      number_code = re.search(r'[A-Z]+(\d+)', partNumber).group(1)
      search_response = requests.get('https://allegromicro.com/all-api/search?q=' + number_code)
      for result in search_response.json().get('Items'):
        if result.get('url'):
          product_response = requests.get(result.get('url'))

          products_soup = BeautifulSoup(product_response.text, 'lxml')

          item_list = list()
          dsheets = ['https://allegromicro.com' + x.parent.attrs['href'] for x in products_soup.find_all('i', class_="fa fa-file-pdf-o")]
          certs = ['https://allegromicro.com' + x.attrs.get('href', '') for x in products_soup.find_all("a", {'href': re.compile(".+certificates-of-compliance.+")})]

          table = products_soup.find('div', class_ = "table-scroll div2")
          headers = table.find('thead').find_all('th')
          table_header = list()
          for th in headers:
              table_header.append(th.text.strip())

          for row in table.find_all('tr'):

            item = {}
            for n, td in enumerate(row.find_all('td')):
              if table_header[n] == 'Part Number':
                item['PartNumber'] = td.text
              elif table_header[n] == 'Part Composition /RoHS Data':
                item['RoHSData'] = save_to_file(
                  ('https://allegromicro.com' +  td.find('a').attrs.get('href')) if td.find('a') else None,
                  'RoHSData_' + item['PartNumber'] + '.pdf',
                  'allegro')
              elif table_header[n] == 'RoHSCompliant':
                item['RoHSCompliant'] =  td.text
              elif table_header[n] == 'Package Type':
                item['partName'] =  td.text


            if item:
              item['DataSheets'] = [save_to_file(
                ds,
                'DataSheet_' + item['PartNumber'] + ('_' + str(n) if n > 0 else '') + '.pdf',
                'allegro'
              ) for n, ds in enumerate(dsheets)]
              item['CertificateOfCompliance'] = [save_to_file(
                c,
                'CertificateOfCompliance_' + item['PartNumber'] + ('_' + str(n) if n > 0 else '') + '.pdf',
                'allegro'
              ) for n, c in enumerate(certs) ]
              item_list.append(item)
              if item['PartNumber'].lower() == partNumber.lower():
                return item

    except (IndexError, AttributeError, requests.exceptions.MissingSchema):
      print('part number is not found on server')
      return {"status":404}



for index, row in tqdm(df.iterrows(), total=df.shape[0]):
  if row['Results'] == 'Found' or row['Results'] == 'Not Found':
    pass
  else:
    try:
      d = scrap_allegro(row['PartNumber'])
      df.loc[index, d.keys()] = tuple(d.values())

    except Exception as e:
#       print(e)
      df.loc[index,"Results"] = "Not Found"
      
df.to_csv(path,index=False)

if __name__ == '__main__':
    print(
      scrap_allegro('A5950GEUSR-T'),
      scrap_allegro('ACS37002LMABTR-050U5'),
      scrap_allegro('ARG81800KESJSR-1')
    )